{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "id": "4858e9a7-3e0e-443a-825b-080ba4ecd579",
      "cell_type": "code",
      "source": "import math\nimport time\nimport numpy as np\nfrom numpy.linalg import norm\nimport matplotlib.pyplot as plt\nimport sys",
      "metadata": {
        "trusted": true
      }
    },
    {
      "id": "1c2230bf-18c1-40c2-8a6a-5c1d86705948",
      "cell_type": "markdown",
      "source": "# Programmieraufgabe: CG-Verfahren und PCG-Verfahren\n\nBetrachtet wird das lineare Gleichungssystem $Ax = b$, wobei\n$$\n A =  \\begin{bmatrix}\n        \\phantom{-}2 & -1 &  &  &  \\\\\n        -1& \\phantom{-}2 & -1&  & \\\\\n        & \\ddots & \\ddots & \\ddots & \\\\\n        & & -1 & \\phantom{-}2 & -1 \\\\\n        & & & -1 & \\phantom{-}2 \n    \\end{bmatrix} \\in \\mathbb{R}^{n\\times n}.\n$$\nverwendet wird. Ziel ist es, dieses System mithilfe des **CG-Verfahrens** (konjugierte Gradienten) und des **PCG-Verfahrens** (vorkonditionierte Gradienten) zu lösen.",
      "metadata": {}
    },
    {
      "id": "6a07d1cd-c451-4a3f-a121-a01797e349e1",
      "cell_type": "markdown",
      "source": "## Teil 1: Implementierung der Hilfsfunktionen\n\nZunächst sind die folgenden Funktionen zu implementieren, die eine explizite Speicherung der vollständigen Matrizen vermeiden:\n\n* `Av = Av_mul(v)`: Diese Funktion soll die Multiplikation der Matrix $A$ mit einem beliebigen Vektor $v$ effizient realisieren, **ohne** dass $A$ als $n \\times n$-Matrix gespeichert werden muss. (Wird auch als \"Matrix-Free\" bezeichnet.)\n* `w = pre_mul(r)`: Analog soll diese Funktion die Multiplikation eines Vektors $r$ mit dem Vorkonditionierer $M^{-1} = S^{-1} S^{-T}$ realisieren.\n\nFür die Vorkonditionierung soll, wie in der Vorlesung besprochen, $S = \\frac{1}{2} D + L^T$ verwendet werden, wobei die Zerlegung $A = L + D + L^T$ zugrunde liegt ($L$ ist der strikte untere Dreiecksteil und $D$ die Diagonale von $A$). Dadurch hat $S^{-T}S^{-1}$ die Form\n$$\n\\begin{bmatrix}\nn & \\dots & 3 & 2 & 1 \\\\\n\\vdots & & \\vdots & \\vdots & \\vdots \\\\\n3 & \\dots & 3 & 2 & 1 \\\\\n2 & \\dots & 2 & 2 & 1 \\\\\n1 & \\dots & 1 & 1 & 1\n\\end{bmatrix}\n$$",
      "metadata": {}
    },
    {
      "id": "d10bfd72-6742-48bf-a2e2-ee6958728dab",
      "cell_type": "code",
      "source": "def Av_mul(v):\n    \"\"\"\n    A*v = Av_mul ( v )\n    (Hier darf nicht die Matrix A angelegt werden.)\n    \"\"\"\n    Av = 2.0 * v\n    Av[:-1] -= v[1:]\n    Av[1:] -= v[:-1]\n    return Av\n\ndef pre_mul(r):\n    \"\"\"\n    Anwendung des Vorkonditionierers S^(-T)S^(-1)\n    (Auch hier dürfen weder A noch S^(-1) als Matrix angelegt werden)\n    \"\"\"\n    n = r.shape[0]\n    z = np.zeros(n)\n    total = 0.0\n    for j in range(0, n):\n        total = total + r[j]\n        z[j] = total\n    w = np.zeros(n)\n    total = 0.0\n    for i in range(n - 1, -1, -1):\n        total = total + z[i]\n        w[i] = total\n    return w\n",
      "metadata": {
        "trusted": true
      }
    },
    {
      "id": "d9cba2cd-80c3-436f-ac17-9628787d96bc",
      "cell_type": "markdown",
      "source": "### Teil 2: Implementierung der CG-Verfahren\n\nAls Nächstes werden die Hauptfunktionen für die iterativen Lösungsverfahren benötigt:\n\n* `x, res = cg(b, avm, tol, kmax)`\n* `x, res = pcg(b, avm, prm, tol, kmax)`\n\nDiesen Funktionen werden `Av_mul` und `pre_mul` als Parameter (`avm` und `prm`) übergeben, um die Matrixoperationen durchzuführen.\n\nParameter:\n* `tol` ist die Fehlertoleranz. Die Verfahren sollen abgebrochen werden, wenn **sowohl** die Norm des Residuums **als auch** die Norm der Differenz aufeinanderfolgender Iterierter $\\leq$ `tol` ist.\n* `kmax` ist die maximale Anzahl an Iterationen. Die Verfahren müssen spätestens terminieren, wenn diese Grenze erreicht ist.\n* Als Startvektor wählen Sie die rechte Seite $b$.\n\n\nDie Rückgabewerte der Funktionen `cg` und `pcg` sollen sein:\n* `x`: Die vom jeweiligen Verfahren berechnete Lösung.\n* `res` (Vektor): Der Vektor mit den Normen des Residuums:\n    $$\\text{res}(i) = \\| b - A x^{(i)} \\|_{2}$$\n",
      "metadata": {}
    },
    {
      "id": "c7bac80a-11d3-4c97-a7b4-3a04ff7c596a",
      "cell_type": "code",
      "source": "def cg(b, avm, tol, max_iter):\n    \"\"\"\n    Konjugierte-Gradienten-Verfahren (CG)\n        solution, residuals, steps = cg(b, avm, tol, max_iter)\n\n    Parameter:\n      b        : Rechte Seite und Initialschätzung (Startvektor x0);\n      avm      : Funktion, die A*v berechnet;\n      tol      : Geforderte absolute Genauigkeit;\n      max_iter : Maximale Zahl von Iterationen.\n\n    Resultate:\n      solution  : Näherungslösungsvektor xk von A*xk=b;\n      residuals : Folge der Residuennormen ||b - A*x^(i)||_2;\n      steps     : Folge der Schrittweitennormen ||x^(i) - x^(i-1)||_2.\n      residuals und steps haben gleiche Länge. steps[0] ist np.nan.\n\n    Das Verfahren bricht ab, wenn entweder die maximale Zahl von\n    Iterationsschritten 'kmax' erreicht ist oder die 2-Normen von\n    Residuum (A*x - b) und Schrittweite (x^(i+1) - x^(i)) beide kleiner-gleich\n    der Toleranz 'tol' sind.\n    \"\"\"\n    n = b.shape[0]\n\n    residuals = np.zeros(max_iter + 1)  # Residuum  || A*x^(i) - b ||_2\n    \n    # Initialisierung\n    iter_count = 0                           # Zählt die berechneten Iterationsschritte\n    solution = np.copy(b)                    # x_0 = b\n    residual = b - avm(solution)             # r_0 = b - A*x_0\n    search_direction = np.copy(residual)     # p_0 = r_0\n    gamma_old = np.inner(residual, residual) # gamma_0 = <r_0, r_0>\n\n    residuals[iter_count] = math.sqrt(gamma_old) \n\n    while (iter_count < max_iter) and (\n        (residuals[iter_count] > tol) or (step_norm > tol)\n    ):\n        # 1. Berechne A*p_k\n        v = avm(search_direction)\n        \n        # 2. Berechne alpha_k (Schrittweite)\n        omega = np.inner(search_direction, v) # <p_k, A*p_k>\n        \n        if abs(omega) < 1e-14:\n            print(\n                f\"CG-Warnung: k={iter_count}/{max_iter} , gamma_old={format(gamma_old,'.2e')} , <p_k,v_k>={format(omega,'.2e')} < 1e-14 !!\"\n            )\n            break\n\n        alpha = gamma_old / omega # alpha_k = gamma_k / omega_k\n\n        # 3. Update x_k+1\n        step_vector = alpha * search_direction\n        step_norm = norm(step_vector)\n        solution += step_vector\n\n        # 4. Update r_k+1\n        residual -= alpha * v\n\n        # 5. Berechne gamma_k+1\n        gamma_new = np.inner(residual, residual) # gamma_k+1 = <r_k+1, r_k+1>\n\n        # 6. Update p_k+1\n        beta = gamma_new / gamma_old # beta_k = gamma_k+1 / gamma_k\n        \n        search_direction *= beta\n        search_direction += residual\n        \n        # 7. Speichern und Update\n        iter_count += 1\n        \n        # Residuen-Norm für den Abbruch: ||r_k+1||_2\n        residuals[iter_count] = norm(residual)\n        \n        gamma_old = gamma_new\n\n    # Die resultierenden Vektoren sollen genau die richtige Laenge haben;\n    # alle ueberschuessigen Elemente werden abgeschnitten:\n    residuals = residuals[: iter_count + 1]\n\n    return (solution, residuals)",
      "metadata": {
        "trusted": true
      }
    },
    {
      "id": "56c6b39d-00bf-4816-aa48-63614b3a44b0",
      "cell_type": "code",
      "source": "def pcg(b, avm, prm, tol, max_iter):\n    \"\"\"\n    Präkonditioniertes Konjugierte-Gradienten-Verfahren (PCG)\n        solution, residuals, steps = pcg(b, avm, prm, tol, max_iter)\n\n    Parameter:\n      b : Rechte Seite und Initialschätzung (Startvektor x0);\n      avm : Funktion, die A*v berechnet;\n      prm : Funktion, die M^(-1)*v berechnet (M^(-1) = S^(-T)*S^(-1));\n      tol : Geforderte absolute Genauigkeit;\n      max_iter: Maximale Zahl von Iterationen.\n\n    Resultate:\n      solution   : Näherungslösungsvektor xk von A*xk=b;\n      residuals : Folge der Residuennormen ||b - A*x^(i)||_2;\n      steps     : Folge der Schrittweitennormen ||x^(i) - x^(i-1)||_2.\n      residuals und steps haben gleiche Länge. steps[0] ist np.nan.\n    \"\"\"\n    n = b.shape[0]\n\n    # Initialisierung der Ergebnis-Vektoren (mit Platz für den Startwert, Index 0)\n    residuals = np.zeros(max_iter + 1)  # Residuum  || A*x^(i) - b ||_2\n\n    # Initialisierung\n    iter_count = 0                     # Zählt die berechneten Iterationsschritte\n    solution = np.copy(b)              # x_0 = b\n    residual = b - avm(solution)       # r_0 = b - A*x_0\n    \n    # Präkonditionierung: w_0 = M^(-1) * r_0\n    preconditioned_residual = prm(residual) # w_0 = M^(-1) * r_0\n    \n    # Suchrichtung: p_0 = w_0\n    search_direction = np.copy(preconditioned_residual) \n    \n    # Skalarprodukt: gamma_0 = <w_0, r_0>\n    gamma_old = np.dot(preconditioned_residual, residual) \n\n    # Speichern der Werte für Iteration k=0\n    residuals[iter_count] = norm(residual) \n\n    # Iteration\n    while (iter_count < max_iter) and (\n        (residuals[iter_count] > tol) or (step_norm > tol)\n    ):\n        # 1. Berechne A*p_k\n        vec_v = avm(search_direction)\n        \n        # 2. Berechne alpha_k (Schrittweite)\n        # omega = <p_k, A*p_k>\n        omega = np.dot(search_direction, vec_v) \n        \n        if abs(omega) < 1e-14:\n            print(\n                f\"PCG-Warnung: k={iter_count}/{max_iter} , gamma_old={format(gamma_old,'.2e')} , <p_k,v_k>={format(omega,'.2e')} < 1e-14!\"\n            )\n            break\n\n        alpha = gamma_old / omega # alpha_k = gamma_k / omega_k\n\n        # 3. Update x_k+1\n        step_vector = alpha * search_direction\n        step_norm = norm(step_vector)\n        solution += step_vector\n\n        # 4. Update r_k+1\n        residual -= alpha * vec_v\n        \n        # 5. Präkonditionierung: w_k+1 = M^(-1) * r_k+1\n        preconditioned_residual = prm(residual)\n        \n        # 6. Berechne gamma_k+1\n        gamma_new = np.dot(preconditioned_residual, residual) # gamma_k+1 = <w_k+1, r_k+1>\n\n        # 7. Update p_k+1\n        beta = gamma_new / gamma_old # beta_k = gamma_k+1 / gamma_k\n        \n        search_direction *= beta\n        search_direction += preconditioned_residual\n        \n        # 8. Speichern und Update\n        iter_count += 1\n\n        # Residuen-Norm für den Abbruch: ||r_k+1||_2\n        residuals[iter_count] = norm(residual)\n        \n        gamma_old = gamma_new\n\n    # Die resultierenden Vektoren sollen genau die richtige Laenge haben;\n    # alle ueberschuessigen Elemente werden daher abgeschnitten:\n    residuals = residuals[: iter_count + 1]\n\n    return (solution, residuals)",
      "metadata": {
        "trusted": true
      }
    },
    {
      "id": "91a4622f-d622-49ab-b4df-942eaadaaaf6",
      "cell_type": "markdown",
      "source": "### Teil 3: Vergleich der Verfahren\n\nNun wollen wir die implementierten Verfahren vergleichen. Dafür betrachten wir verschieden große Matrizen $A \\in \\mathbb{R}^{n \\times n}$, wobei wir $n = 2^m + 1$ setzen. Für $m = 3$ ist $A$ also eine $9 \\times 9$-Matrix, für $n=10$ hingegen schon $1025 \\times 1025$. \n\nTesten Sie ihre Verfahren, in dem Sie die folgende Zelle ausführen:",
      "metadata": {}
    },
    {
      "id": "0c144d3e-8d5e-4ce3-81b8-90ccf10f9bb3",
      "cell_type": "code",
      "source": "from util.plotting_06 import compute_and_plot_06\n\ntol = 1.0e-9\nm_smallest = 3\nm_largest  = 12\n\nn_iterations = np.zeros((m_largest - m_smallest, 2), dtype=int)\nresiduals = np.zeros((m_largest - m_smallest, 2))\nnorms = np.zeros((m_largest - m_smallest, 2))\ntimes = np.zeros((m_largest - m_smallest, 3))\n\nfor i, m in enumerate(range(m_smallest, m_largest)):\n    (\n        cg_solution,\n        cg_residuals,\n        cg_norm,\n        cg_time,\n        pcg_solution,\n        pcg_residuals,\n        pcg_norm,\n        pcg_time,\n        init_time,\n    ) = compute_and_plot_06(Av_mul, pre_mul, cg, pcg, m, tol, 2**m)\n    if (type(cg_solution) == np.ndarray):\n        n_iterations[i, 0] = cg_residuals.shape[0]\n        residuals[i, 0] = cg_residuals[-1]\n        norms[i, 0] = cg_norm\n        times[i, 0]  = cg_time\n    else:\n        n_iterations[i, 0] = 0\n        residuals[i, 0] = np.inf\n        norms[i, 0] = np.inf\n        times[i, 0]  = np.inf\n    n_iterations[i, 1] = pcg_residuals.shape[0]\n    residuals[i, 1] = pcg_residuals[-1]\n    norms[i, 1] = pcg_norm\n    times[i, 1]  = pcg_time\n    times[i, 2]  = init_time\n\n# Ausgabe der Tabellen\nnames = [\"CG\", \"PCG\"]\nfor i in range(2):\n    if i == 1 and (type(cg_residuals) != np.ndarray):\n        continue\n    print(f\"{names[i]}-Verfahren\")\n    print('m\\titer\\t||x-u||\\t\\t||b-Ax||\\tct[\"]\\t\\tict[\"]')\n    for j, m in enumerate(range(m_smallest, m_largest)):\n        print(f\"{m}\\t{n_iterations[j,i]}\\t{format(norms[j, i], '.2e')}\\t{format(residuals[j, i], '.2e')}\\t{format(times[j, i], '.1e')}\\t\\t{format(times[j, 2], '.1e')}\")\n    print(\"\")",
      "metadata": {
        "trusted": true
      }
  }
  ]
}
